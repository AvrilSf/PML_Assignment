---
title: "HAR-WLE"
author: "Monica Maria Solorzano Rodriguez"
date: "12/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Human Activity Recongnition - Weight Lifthing Excercise, HAR-WLE

Human activity recognition utilizes sensors, smartphones or images to track physical activity of persons for purposes like energy monitoring and  usage, mostly in the field of health care. Papers started being published in 2006 as per records of the Institute of Electrical and Electronics Engineers. If you now go to their website and search for these key words, you get 422 million documents you can have access to in this and other related websites. In order to process and evaluate the data generated by the devices, there are various machine learning techniques used, for example: decision trees, K-nearest neighbors, support vector machines, hidden Markov models, among others. The data set to be used in the present assignment belongs to the "Qualitative Activity Recognition of Weight Lifting Excercises" research paper by Velloso, E., Bulling, A., Gellersen, H., Ugulino, W. and Fuks, H. (Proceedings of the 4th International Conference, Stuttgart, Germany: ACM SIGCHI, 2013), which investigates "how well" unilateral dumbbell biceps curl is performed, see <http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har>.

Let's take a look at some features of the data set, it can be seen that some features are easily correlated whereas some others definitely not:

```{r WLE, echo=FALSE, error=FALSE, message=FALSE, warning=FALSE, comment="", cache=TRUE}
library(caret)
library(doParallel)
library(parallel)
library(snow)
TRurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TEurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(TRurl, destfile = "./pml-training.csv", method = "curl")
download.file(TEurl, destfile = "./pml-testing.csv", method = "curl")
training <- read.csv("./pml-training.csv", header = TRUE, sep = ",")
testing <- read.csv("./pml-testing.csv", header = TRUE, sep = ",")

#validation set
set.seed(54321)
inTraining <- createDataPartition(training$classe, p = 0.999, list = FALSE)
training2 <- training[inTraining, ]
validation <- training[-inTraining, ]

#feature plot
featurePlot(x = training2[, c(50, 66, 67, 68, 102, 113, 114, 115, 119, 120, 121)], y = factor(training2$classe), plot = "pairs")
```   
   
   Taking a close look to all the variables, it can be seen that there are 33 variables that are in mode = character and which cannot be processed:

```{r WLE2, echo=FALSE, error=FALSE, message=FALSE, warning=FALSE, comment="", cache=TRUE}
library(caret)
library(doParallel)
library(parallel)
library(dplyr)
library(snow)
TRurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TEurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(TRurl, destfile = "./pml-training.csv", method = "curl")
download.file(TEurl, destfile = "./pml-testing.csv", method = "curl")
training <- read.csv("./pml-training.csv", header = TRUE, sep = ",")
testing <- read.csv("./pml-testing.csv", header = TRUE, sep = ",")

#validation set
set.seed(54321)
inTraining <- createDataPartition(training$classe, p = 0.999, list = FALSE)
training2 <- training[inTraining, ]
validation <- training[-inTraining, ]

#cleaning data
#columns with mode character
str(training[,  c(12:17, 20, 23, 26, 69:74, 87:92, 95, 98,101, 125:130, 133, 136, 139)] )
```

There are also 67 variables that have over 97% of NAs in them.  Therefore, it was necessary to preform some data cleaning. Have a look:

```{r WLE3, echo=FALSE, error=FALSE, message=FALSE, warning=FALSE, comment="", cache=TRUE}
library(caret)
library(doParallel)
library(parallel)
library(dplyr)
library(snow)
TRurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TEurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(TRurl, destfile = "./pml-training.csv", method = "curl")
download.file(TEurl, destfile = "./pml-testing.csv", method = "curl")
training <- read.csv("./pml-training.csv", header = TRUE, sep = ",")
testing <- read.csv("./pml-testing.csv", header = TRUE, sep = ",")

#validation set
set.seed(54321)
inTraining <- createDataPartition(training$classe, p = 0.999, list = FALSE)
training2 <- training[inTraining, ]
validation <- training[-inTraining, ]

#cleaning data
#columns with mode character in training[,  c(12:17, 20, 23, 26, 69:74, 87:92, 95, 98,101, 125:130, 133, 136, 139)] 

#finding NA columns in train and selecting non NA columns --- object doesn't work
mn <- data.frame()
for (i in 1:160) { 
  cm = mean(training2[, i])
  mn <- rbind(mn, cm)
}
n <- 1:160
mnn <- cbind(mn, n)

# columns with mode "character" that can't be computed are: training[,  c(12:17, 20, 23, 26, 69:74, 87:92, 95, 98,101, 125:130, 133, 136, 139)] 

na98 <- data.frame()
for (i in 1:160) { 
  tr = table(is.na(training2[, i]))
  na98 <- rbind(na98, tr)
}

NApercent <- data.frame()
for (i in 1:160) { 
  NAp <- (1 - na98[i, 1]/length(training2$X)) * 100
  NApercent <- rbind(NApercent, NAp)
}
tail(NApercent, 20)
```

Finally, the variables that can be used as regressors are the following, note that the first three will not be used, the first one is the number of row and the other two are dates, and it doesnÂ´t apply here because we are not forecasting.

```{r WLE4, echo=FALSE, error=FALSE, message=FALSE, warning=FALSE, comment="", cache=TRUE}
library(caret)
library(doParallel)
library(parallel)
library(dplyr)
library(snow)
TRurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TEurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(TRurl, destfile = "./pml-training.csv", method = "curl")
download.file(TEurl, destfile = "./pml-testing.csv", method = "curl")
training <- read.csv("./pml-training.csv", header = TRUE, sep = ",")
testing <- read.csv("./pml-testing.csv", header = TRUE, sep = ",")

#validation set
set.seed(54321)
inTraining <- createDataPartition(training$classe, p = 0.999, list = FALSE)
training2 <- training[inTraining, ]
validation <- training[-inTraining, ]

#cleaning data
#columns with mode character in training[,  c(12:17, 20, 23, 26, 69:74, 87:92, 95, 98,101, 125:130, 133, 136, 139)] 

#finding NA columns in train and selecting non NA columns --- object doesn't work
mn <- data.frame()
for (i in 1:160) { 
  cm = mean(training2[, i])
  mn <- rbind(mn, cm)
}
n <- 1:160
mnn <- cbind(mn, n)

# columns with mode "character" that can't be computed are: training[,  c(12:17, 20, 23, 26, 69:74, 87:92, 95, 98,101, 125:130, 133, 136, 139)] 

na98 <- data.frame()
for (i in 1:160) { 
  tr = table(is.na(training2[, i]))
  na98 <- rbind(na98, tr)
}

NApercent <- data.frame()
for (i in 1:160) { 
  NAp <- (1 - na98[i, 1]/length(training2$X)) * 100
  NApercent <- rbind(NApercent, NAp)
}
# columns with 97.89% of NAs that cannot be used as regressors are: training[, c(18:19, 21:22, 24:25, 27:36, 50:59, 75:83, 93:94, 96:97, 99:100, 103:112, 131:132, 134:135, 137:138, 141:150)]

nNAc <- filter(mnn, !is.na(mnn[, 1]))
co <- nNAc[, 2]
predictors <- co[4:56]
# columns without NAs that can be used as regressors: training[, c(1, 3, 4, 7,  8, 9, 10, 11, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 60, 61, 62, 63, 64, 65, 66, 67, 68, 84, 85, 86, 102, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 140, 151, 152, 153, 154, 155, 156, 157, 158, 159)]
# Note that the first three will not be used as they are row numbers and dates (this is not a forecast).
predictors
```

There were 6 models fitted starting from k-fold 3 and up until k-fold 8.  They were tested on the validation set partitioned right after reading the files and since it only has 17 observations to try to resemble the testing set, they all predicted successfully.  The accuracy of the selected fit model is 0.9985 so the expected out of sample error in the training set should be very small, tending to zero. The resampling method utilized was k-fold cross validation, the modeling method chosen was random forest.  This selection was made considering different aspects, the first of them is that the outcome is a factor variable, additionally, the large amount of variables and observations in the training set, and also the small amount of observations in the test set. It is believed that with an 8 fold cross validation the bias and the variability will be balanced, because it is not so large that would have a very high variability, neither too small that would negatively affect the accuracy of the prediction. In other words a high accuracy is needed for such a small testing data set.  The results of the fit are as follows:

```{r WLE5, echo=FALSE, error=FALSE, message=FALSE, warning=FALSE, comment="", cache=TRUE}
library(caret)
library(doParallel)
library(parallel)
library(dplyr)
library(snow)
TRurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TEurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(TRurl, destfile = "./pml-training.csv", method = "curl")
download.file(TEurl, destfile = "./pml-testing.csv", method = "curl")
training <- read.csv("./pml-training.csv", header = TRUE, sep = ",")
testing <- read.csv("./pml-testing.csv", header = TRUE, sep = ",")

#validation set
set.seed(54321)
inTraining <- createDataPartition(training$classe, p = 0.999, list = FALSE)
training2 <- training[inTraining, ]
validation <- training[-inTraining, ]

#cleaning data
#columns with mode character in training[,  c(12:17, 20, 23, 26, 69:74, 87:92, 95, 98,101, 125:130, 133, 136, 139)] 

#finding NA columns in train and selecting non NA columns --- object doesn't work
mn <- data.frame()
for (i in 1:160) { 
  cm = mean(training2[, i])
  mn <- rbind(mn, cm)
}
n <- 1:160
mnn <- cbind(mn, n)

# columns with mode "character" that can't be computed are: training[,  c(12:17, 20, 23, 26, 69:74, 87:92, 95, 98,101, 125:130, 133, 136, 139)] 

na98 <- data.frame()
for (i in 1:160) { 
  tr = table(is.na(training2[, i]))
  na98 <- rbind(na98, tr)
}

NApercent <- data.frame()
for (i in 1:160) { 
  NAp <- (1 - na98[i, 1]/length(training2$X)) * 100
  NApercent <- rbind(NApercent, NAp)
}
# columns with 97.89% of NAs that cannot be used as regressors are: training[, c(18:19, 21:22, 24:25, 27:36, 50:59, 75:83, 93:94, 96:97, 99:100, 103:112, 131:132, 134:135, 137:138, 141:150)]

nNAc <- filter(mnn, !is.na(mnn[, 1]))
co <- nNAc[, 2]
predictors <- co[4:56]
# columns without NAs that can be used as regressors: training[, c(1, 3, 4, 7,  8, 9, 10, 11, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 60, 61, 62, 63, 64, 65, 66, 67, 68, 84, 85, 86, 102, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 140, 151, 152, 153, 154, 155, 156, 157, 158, 159)]
# Note that the first three will not be used as they are row numbers and dates (this is not a forecast).
y = training2[, 160]
x = training2[, c(7, 8, 9, 10, 11, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 60, 61, 62, 63, 64, 65, 66, 67, 68, 84, 85, 86, 102, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 140, 151, 152, 153, 154, 155, 156, 157, 158, 159)]

# training
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
set.seed(12345)
fitControl <- trainControl(method = "cv", number = 8, allowParallel = TRUE, seeds = NULL)
fit <- train(x, y, method = "rf", data = training2, trControl = fitControl)
stopCluster(cluster)

#validation
validation2 = validation[, c(7, 8, 9, 10, 11, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 60, 61, 62, 63, 64, 65, 66, 67, 68, 84, 85, 86, 102, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 140, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160)]

# testing
testing2 = testing[, c(7, 8, 9, 10, 11, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 60, 61, 62, 63, 64, 65, 66, 67, 68, 84, 85, 86, 102, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 140, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160)]

fit
fit$resample
confusionMatrix.train(fit)

```
## CODE

```{r WLE6, echo=TRUE, eval=FALSE}
library(caret)
library(doParallel)
library(parallel)
library(dplyr)
library(snow)
TRurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TEurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(TRurl, destfile = "./pml-training.csv", method = "curl")
download.file(TEurl, destfile = "./pml-testing.csv", method = "curl")
training <- read.csv("./pml-training.csv", header = TRUE, sep = ",")
testing <- read.csv("./pml-testing.csv", header = TRUE, sep = ",")

#validation set
set.seed(54321)
inTraining <- createDataPartition(training$classe, p = 0.999, list = FALSE)
training2 <- training[inTraining, ]
validation <- training[-inTraining, ]

#cleaning data
#columns with mode character in training[,  c(12:17, 20, 23, 26, 69:74, 87:92, 95, 98,101, 125:130, 133, 136, 139)] 

#finding NA columns in train and selecting non NA columns --- object doesn't work
mn <- data.frame()
for (i in 1:160) { 
  cm = mean(training2[, i])
  mn <- rbind(mn, cm)
}
n <- 1:160
mnn <- cbind(mn, n)

# columns with mode "character" that can't be computed are: training[,  c(12:17, 20, 23, 26, 69:74, 87:92, 95, 98,101, 125:130, 133, 136, 139)] 

na98 <- data.frame()
for (i in 1:160) { 
  tr = table(is.na(training2[, i]))
  na98 <- rbind(na98, tr)
}

NApercent <- data.frame()
for (i in 1:160) { 
  NAp <- (1 - na98[i, 1]/length(training2$X)) * 100
  NApercent <- rbind(NApercent, NAp)
}
# columns with 97.89% of NAs that cannot be used as regressors are: training[, c(18:19, 21:22, 24:25, 27:36, 50:59, 75:83, 93:94, 96:97, 99:100, 103:112, 131:132, 134:135, 137:138, 141:150)]

nNAc <- filter(mnn, !is.na(mnn[, 1]))
co <- nNAc[, 2]
predictors <- co[4:56]
# columns without NAs that can be used as regressors: training[, c(1, 3, 4, 7,  8, 9, 10, 11, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 60, 61, 62, 63, 64, 65, 66, 67, 68, 84, 85, 86, 102, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 140, 151, 152, 153, 154, 155, 156, 157, 158, 159)]
# Note that the first three will not be used as they are row numbers and dates (this is not a forecast).
y = training2[, 160]
x = training2[, c(7, 8, 9, 10, 11, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 60, 61, 62, 63, 64, 65, 66, 67, 68, 84, 85, 86, 102, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 140, 151, 152, 153, 154, 155, 156, 157, 158, 159)]

# training
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
set.seed(12345)
fitControl <- trainControl(method = "cv", number = 8, allowParallel = TRUE, seeds = NULL)
fit <- train(x, y, method = "rf", data = training2, trControl = fitControl)
stopCluster(cluster)

#validation
validation2 = validation[, c(7, 8, 9, 10, 11, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 60, 61, 62, 63, 64, 65, 66, 67, 68, 84, 85, 86, 102, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 140, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160)]

# testing
testing2 = testing[, c(7, 8, 9, 10, 11, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 60, 61, 62, 63, 64, 65, 66, 67, 68, 84, 85, 86, 102, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 140, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160)]

fit
fit$resample
confusionMatrix.train(fit)

```
